---
title: "Word Vectors Basics"
author: "Jonathan Fitzgerald & Sarah Connell"
date: "1/1/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Getting started 

## Using this file

This file is a first introduction to word vectors; we'll use it to understand the basics of querying models, before we move on to training models of our own.   

## Reminder on running code
To run a single line of code from an R Markdown file, put your cursor anywhere in that line of code and then hit `command-return` or `control-return.` If you want to run all of the code in a code snippet, you can hit the green triangle button on the right. If you want to run a particular section of code, select the section you want to run and hit `command-return` or `control-return`. 

You can also run code directly in the console (the window below where you are reading this) by typing or pasting it in and simply hitting `return`. You will get the same results, but if you want to save some code you've written, it's better to keep it in the R Markdown file, since that's more permanent. On the other hand, if you prefer to run some code but not make changes to your file, you can just run that in the console.

##Checking working directory
If this is a new RStudio session, you should check your working directory with the code below. As long as you opened this file from the WordVectors project, your working directory should be the right place: the "WordVectors" folder. If you do need to change your working directory, you can do so with the `setwd()` function.

```{r}

getwd()

```

## Loading packages
All the packages you will need for this exercise have been installed ahead of time, but you'll want to load them using the `library()` function if this is a new session. You'll have to load these packages every time you start a new session in RStudio.

```{r}

library(tidyverse)
library(tidytext)
library(magrittr)
library(devtools)
library(tsne)
library(wordVectors)

```

#Working with word2vec models

## Reading in existing models

Before you train models of your own, you'll start by reading in existing models. We've provided a set of sample models (all the files that end in '.bin') for you in the `data` folder; to read them in, just edit and run the line of code below. You can also use this code to read in models that you've trained (once you have some!) if you start a new session—and you can even read in models trained by others, if you save their .bin file in your `data` folder. 

We've filled in sample model to get you started. If you want to see a different model, the *only* thing you'll need to change in the code below is the file path. Navigating file paths can be a bit confusing, but fortunately there is a trick you can use. Try deleting the text inside of the quotation marks below (but don't delete the quotation marks!) and then hitting `tab`. You should get a pop-up with a view of the folder system you're more used to navigating; you can use this to fill in file paths whenever you need to.

```{r}

w2vModel = read.vectors("data/eighteenth_c_collections_online.bin")

```

## Visualizing models 

We can get a glimpse of what the model looks like by plotting it in two dimensions. Keep in mind that the model actually has many more dimensions, so we are, in effect, flattening it. Though the visualization is difficult to read, you should be able to see that similar words—words that are near each other in vector space—tend to clump together. The code below will likely take a minute or two to run, and your results will appear in the "Plots" window to the right (you can hit the "Zoom" button to get a better view).

```{r}

w2vModel %>% plot(perplexity=10)

```

## Clustering

The following script provides a way to cluster words that are near each other in vector space, using the "k-means" clustering algorithm. Below, we choose 150 `centers`, or 150 points around which to cluster words. Then we select ten random clusters and 15 words from each cluster to view. This code will also take a minute or two to run. You can change the number of centers, number of clusters to view, or the number of words to see—you can also increase the number of iterations (the number of times the algorithm should adjust where the centers are and where terms are positioned in relationship to those centers).

```{r}

centers <- 150
clustering <- kmeans(w2vModel,centers=centers,iter.max = 40)

sapply(sample(1:centers,10),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:15])
})

```

## Closest to

Now that you've had a chance to think about the corpus as a whole, it's time to start investigating individual words. To find the words closest to a particular word in vector space, enter a term between the quotation marks and then run the code below. You'll notice the output shows up in the console. If you want to see more words, just increase the number. Make sure not to delete the quotation marks, and enter your word in lowercase. 


```{r}

w2vModel %>% closest_to('man', 10) 

```

## Closest to two things

You might also want to see the words closest to a combination of two (or more) words. Notice that this will open a new window with the results because of the `view()` function. If you prefer to see your results in this format, you can paste "%>% view()" at the end of the code above; or, if you prefer to see your results in the console, you can delete "%>% View()" from the code below. Note that the code below also shows just 20 results, instead of 30.

```{r}

w2vModel %>% closest_to(~"girl"+"woman", 20) %>% view()

```


## Closest to the space between two things
Or, you might want to look at the space between two terms, to see which words are similar to one term but not another: 

```{r}

w2vModel %>% closest_to(~'woman'-'man',20) 

```

In our next walkthrough, we'll cover training models, as well as a few other things you can do with word2vec.

#Credit and thanks
This tutorial uses the `wordVectors` package developed by Ben Schmidt and Jian Li, itself based on the original `word2vec` code developed by Mikolov et al. The walkthrough was also informed by workshop materials authored by Schmidt, as well as by an exercise created by Thanasis Kinias and Ryan Cordell for the "Humanities Data Analysis" course.


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Word Vectors in Python\n",
    "\n",
    "This notebook builds off of the core notebook and analysis notebook and assumes that you have completed both before proceeding to this one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Algorithm Should I Use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** is a popular natural language processing library that is usually used for topic modelling. Gensim comes with the popular Word2Vec algorithm\n",
    "\n",
    "**Spacy** is also a popular natural language processing library that is designed to be very fast. Spacy also uses Word2Vec style word embeddings, but tends to be slightly faster than Gensim. Spacy also comes with pre-trained models built in which is incredibly useful if you are wanting to get familiar with querying a model before building your own. \n",
    "\n",
    "**GloVe** is an unsupervised learning algorithm developed by Stanford University. GloVe comes with some nice pre-trained models if you want to play around with word embeddings without having to train your own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Use Gensim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a very memory-efficient way to work with word embedding models. Not only does Gensim come with some cool algorithms that you can apply to a downstream task such as topic modelling, but Gensim also allows you to process large amounts of text without storing them into memory. Developed by Radim Řehůřek, Gensim is one of the most popular libraries for training word embeddding models in Python. Its popularity is an important feature because that means there is a vast amount of community support for the library, making troubleshooting very easy. \n",
    "\n",
    "However, if you want some quick, out of the box models to work with to test things out, then GloVe may be a better choice for your. GloVe and Gensim, however, approach [training](https://machinelearninginterview.com/topics/natural-language-processing/what-is-the-difference-between-word2vec-and-glove/) in different ways which is important to keep in mind if you switch back and forth between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Be More Memory Efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model Vectors\n",
    "\n",
    "Above, I noted that Gensim is a memory efficient way to work with word embedding models. However, just because Gensim is memory efficient does not mean that there aren't little things that you can do to lessen the burden of the model on your machine. One way to make working with word embedding models through Gensim more memory efficient, is to only load the vectors, themselves, rather than the entire model when you are just planning on querying the model. By only loading the vectors, you don't have to load the entire model into memory, and depending on the size of your model, this can be quite the task for your machine. Below, I explain how to load the vectors from a Gensim model.\n",
    "\n",
    "### The Code\n",
    "\n",
    "The first thing that we want to do is actually pull the vectors out of the model. Then, we'll save those vectors as a new file just like the `.model` file so that we can call those vectors later. In this code, we initiate a new variable called `word_vectors` which will hold `model.wv.` `Model.wv` represents the word vectors within the model, itself. Then, we save the word vectors that we have pulled out of the model as a `.wordvectors` file called `word2vec`. Finally, we initialize a variable called `wv` (short for 'word vector') and use the `KeyedVectors()` function to load the wordvectors file and read it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that your model is loaded\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# declare a variable to hold the vectors\n",
    "word_vectors = model.wv\n",
    "\n",
    "# save those vectors to a new file so that we can use them later\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "\n",
    "# now load those vectors\n",
    "# you can now query the model by using \"wv\"\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now just use `wv.` to query the model rather than `model.wv.` `wv` is capable of performing all of the querying functions as `model.wv` can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives the word most similar to 'recipe'\n",
    "wv.most_similar('recipe', topn=10)\n",
    "\n",
    "# gives a cosine similarity for the words 'milk' and 'cream'\n",
    "wv.similarity(\"milk\", \"cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resuming Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with word embedding models is typically an iterative process where you train a model, evaluate it, and then train the model again. Thankfully, Gensim provides the functionality for introducing new data to an existing model. \n",
    "\n",
    "The code below is formatted to read in a list of sentences that are hard-coded, however, if you have a folder of texts or a spreadsheet, you would just use the methods outlined in the core notebook which I won't re-hash here. Essentially, the entire process is the same as before, except instead of declaring a new model we just use the `train()` function that we have built in. \n",
    "\n",
    "### The Code\n",
    "\n",
    "After loading in the libraries we need, the code below begins by loading the current version of our model into memory by using `Word2Vec.load()`. Next, I have declared a list variable with a few sentences from a cupcake recipe in it as strings. If you are using more extensive data, then this is where you would repeat the step of loading in from a folder or spreadsheet from the core notebook. \n",
    "\n",
    "Then, I have the `clean_text()` defined. This function is exactly the same as the `clean_text()` function from the core notebook. I apply the `clean_text()` function to my list in the same way I did in the core notebook. Up until this point, all of these steps are borrowing from the core notebook. \n",
    "\n",
    "Now, we get to the bit that differs from the core notebook. Now that we have a list of tokens, we build the vocabulary for our model by calling `model.build_vocab()`. We are building the vocabulary using the new data and using the `update=True` parameter to let our model know that we are updating the vocabulary in our existing model.\n",
    "\n",
    "Finally, we call the built in function `train()` to retrain the model. Whereas in the core notebook, we used `model= Word2Vec()` to train a new model, by using `model.train()` we tell the model to train using these additional items rather than replacing the vocabulary that the model has already built. If you were to call `model=Word2Vec` instead, you would be overwriting the existing model to only contain the vocabulary of the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim                      # for Word2Vec\n",
    "from gensim.models import Word2Vec # for Word2Vec\n",
    "import re                          # for regular expressions\n",
    "import string                      # for string comprehension\n",
    "\n",
    "# load our current model\n",
    "model = Word2Vec.load(r\"C:\\Users\\avery\\.spyder-py3\\models\\wordvector models\\word2vectest.model\")\n",
    "\n",
    "# declare a variable with our new sentences/words\n",
    "# you can use the folder/spreadsheet method from the core notebook if you have more data\n",
    "more_sentences = [\n",
    "    \"Cup cake is about as good as pound cake, and is cheaper.\", \n",
    "     \"One cup of butter, two cups of sugar, three cups of flour,\", \n",
    "     \"and four eggs, well beat together, and baked in pans or cups.\", \n",
    "     \"Bake twenty minutes, and no more.\"\n",
    "]\n",
    "\n",
    "# we're going to use out clean_text() function from the core notebook\n",
    "def clean_text(text):\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # lower case\n",
    "    tokens = text.split()\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    # remove punctuation\n",
    "    tokens = [re_punc.sub('', token) for token in tokens] \n",
    "    # only include tokens that aren't numbers\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "# declare an empty list to hold our clean text\n",
    "data_clean = []\n",
    "\n",
    "# iterate through the new data and apply the data_clean() to each item\n",
    "for x in more_sentences:\n",
    "    data_clean.append(clean_text(x))\n",
    "\n",
    "# build our model vocab and tell the model that we are updating the vocab\n",
    "model.build_vocab(data_clean, update=True)\n",
    "\n",
    "# tell the model to re-train with the new data\n",
    "model.train(data_clean, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add additional data and retrain a model as many times as you want. Something to keep in mind, however, is that you may to save your model under a different name than the name of the previous model. This way, you'll still have access to the old model in case something goes wrong in the re-training process. To do so, you would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model as a retrained model with the date that it was retrained \n",
    "# you can save your newly trained model under whatever name makes the most sense to you\n",
    "model.save(\"word2vec_retrained_08012022.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also provides the functionality to load an existing model into memory. Using pre-trained models can be incredibly useful if you know of a model that someone else has already trained that suits your needs. For example, the Google News Dataset is freely available to use and is already trained on roughly three million words.\n",
    "\n",
    "### The Code\n",
    "\n",
    "In the code below, we begin by loading the `gensim.downloader` as `download`. Then we declare a variable `wv` and use it to store our call to download the Google dataset. To query the model, you can use all of the queries and analysis functions introduced in the core and analysis notebooks by calling `wv`. Keep in might that the Google model is very large and may take a while to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as download\n",
    "wv = download.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs. Unsupervised vs. Semi-Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to Do With Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Downstream Task?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

title: "Word Vectors Intro"
author: "Jonathan Fitzgerald & Sarah Connell"
date: "6/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Using this File

This file is an introduction to training and querying a model using word2vec.   

This is an R Markdown file, which means that it contains both text (what you're reading now) that can be formatted for display on the web or as a pdf file, and snippets of code, which you can run right from the file. 

To run a single line of code from an R Markdown file, put your cursor anywhere in that line of code and then hit `command-return` or `control-return` (the one exception is defining functions, which we'll cover later on).  If you want to run all of the code in a code snippet, you can hit the green triangle button on the right. If you want to run a particular section of code, select the section you want to run and hit `command-return` or `control-return`.

Much of the code you'll need will run almost instantly, but some things will take a few seconds or minutes (and, depending on the size of your corpus, training a model can take even longer). You can tell code is still running if you see a red stop sign in the top-right corner of the console—if you'd like to stop a process, you can hit this stop sign. You will know that the code has been run when you see a new `>` prompt in the bottom of the console. If you are running code in a block line by line, your cursor will automatically move down to the next line, so you can move through the block by repeatedly hitting `command-return` or `control-return`.

```{r}
print("Put your cursor on this line and try running it!")
print("Try running this entire code snippet!")

```

You can also run code directly in the console (the window below this text) by typing or pasting it in and hitting `return`. You will get the same results, but if you want to save some code you've written, it's better to keep it in the R Markdown file, since that's more permanent. On the other hand, if you'd prefer to run some code but not make changes to this file, you can just run that code in the console. 

If you opened the "WordVectors" project file first, then you should already be working in the "WordVectors" project space. Projects are a way to organize your work in RStudio; as long as you have this project open, your files should be where you expect them to be. Going forward, whenever you open RStudio, it should also open this project. If it does not, however, you can open the project by going to `File` then `Open Project...` in the menu bar at the top of RStudio.

This introduction provides some basic instructions to get you started, but it is not a substitute for learning the fundamentals of R and RStudio. Fortunately, there are many helpful resources online, including tutorials by the [Programming Historian](https://programminghistorian.org/en/lessons/) and Lincoln Mullen's [Digital History Methods in R](https://lincolnmullen.com/projects/dh-r2/).


## Set and Check the Working Directory

You will want to make sure your working directory is in the right place; if you opened this file from within the "WordVectors" project, then it should be, but checking your working directory before you begin a session is always a good idea, because if the working directory is *not* where you're expecting, then not much else will work. Any time you see an error message that says a file does not exist in the current working directory, that's a good sign your working directory isn't where you think it is. 

There are two lines of code in the block below; the first will allow you to check your working directory and the second will allow you to set your working directory, if you ever need to change it. If you run the first line of code and the results in the console show that your working directory is wherever you have saved the "WordVectors" folder on your computer, then you don't need to run the second line of code.

If you ever do need to change your working directory, use the `setwd()` function. As you can see, we've provided you with some template text that you can replace with a file path specific to your computer. 

Navigating file paths can be a bit confusing, but, fortunately there is a trick you can use. If you try deleting the text inside of the quotation marks below and then hitting `tab`, you should get a pop-up with a view of the folder system you're used to navigating; you can use this to fill in file paths whenever you need to (again, you are not likely to need to change your working directory if you opened this file from the "WordVectors" project, but it's good to know how to do this, in case you do need to). 

```{r}
# How to check your working directory (this is also an example of how you add a comment to your code—by typing "#")
getwd()
# How to set your working directory; don't run this unless you need to change your working directory
setwd("Path/To/Your/Directory")

```


## Install Packages

There are also some "packages" you will need to install (make sure you have an internet connection beforehand), so that you'll be able to use functions that don't come with R out of the box. Run each line of code below, just as you did above with `getwd()`. 

```{r}
install.packages("tidyverse")
install.packages("tidytext")
install.packages("magrittr")
install.packages("devtools")
install.packages("tsne")

```


## Load Packages

Now that the packages have been installed, you'll want to load them using the `library()` function. You should only need to install packages once, but you'll have to load them every time you start a new session in RStudio. In general, you'll want to make a habit of checking your working directory and loading your packages at the start of each session.

```{r}
library(tidyverse)
library(tidytext)
library(magrittr)
library(devtools)
library(tsne)
# Hold off on running the line below until after you get to the next section 
library(wordVectors)

```


## Install word2vec

Because the `wordVectors` package lives in GitHub, you will need to install it in a different way, using the `devtools()` function. That's why we had you pause before loading `wordVectors` above. After you've installed the `wordVectors` package (by running the code snippet below), you can ignore or even delete the comment above and just load all the packages at once. 

Make sure to load `wordVectors` after you install it; you can either scroll up and load it from the code block above, or you could try writing the code to load it yourself (either in the code block below or right in the console). 

```{r}
devtools::install_github('bmschmidt/wordVectors', force=TRUE)

```


## Read-in and Combine Multiple Text Files

The following script allows you to "read-in" multiple text files and combine them into a "tibble," which is a type of data table. Think of it as being like a spreadsheet, with rows and columns organizing information.

First, we get a list of the files to read-in (`fileList`), then we create a function (`readTextFiles`) to create a tibble with two columns, `filename` and `text` for each text file in the folder. Then, we run the function to combine everything into one tibble called `combinedTexts`.

There are some special requirements when you want to run code that is defining functions; unlike most of the time, where you can put your cursor anywhere in the line of code to run it, you need to have your cursor either at the beginning or the end of the code defining your function when you run it (or just select the whole thing and run it).

The only thing you'll need to change in the code block below is the file path in the first line; remember that you can use `tab` to navigate to the folder with your text files. As long as you save the folder that has the text files you want to use inside of your "data" folder, you should only need to change the part after the forward slash (the part that reads "name_of_your_folder"). Make sure to change that one line *before* you run any of the code below.

```{r}
# Change "name_of_your_folder" to match the name of the folder with your corpus
path2file <- "data/name_of_your_folder"
fileList <- list.files(path2file, full.names=TRUE) # This will create a list of files in the folder

# This is where you define a function to read-in multiple texts and paste them into a tibble (remember that the code that defines functions must be run by putting your cursor at the beginning or end, or by selecting the whole section of code). You are only defining the function here; the next section of code is when you actually run the function.
readTextFiles <- function(file) { 
  message(file)
  rawText = paste(scan(file, sep="\n", what="raw", strip.white=TRUE))
  output = tibble(filename=gsub(path2file, "", file), text=rawText) %>% 
    group_by(filename) %>% 
    summarise(text = paste(rawText, collapse=" "))
  return(output)
}

# This is where you run the function to create a tibble of combined files
combinedTexts <- tibble(filename=fileList) %>% 
  group_by(filename) %>% 
  do(readTextFiles(.$filename)) 

```


## Prepare Text for word2vec

The section below defines several variables so that they can be called on in training your model. Working with general names (such as "w2vInput") for these variables lets you use them in the code that follows without having to change each instance; the first line is where you set up the specifics you need to distinguish one model from another.

You can pick any name you want in the first line of code below; make sure there are no spaces in the name you select and that it is descriptive enough that you will remember which corpus you were working from when you want to read-in a trained model. 

The only line in the block of code below that you will need to change is the first one, but make sure to do this, or you will end up with a file called "your_file_name.bin"!

```{r}
# This section is where you define the variables you will be using to train your model; don't forget to change the text in the first line to whatever you want to call your model file
baseFile <- "your_file_name"
w2vInput <- paste("data/",baseFile,".txt", sep = "")
w2vCleaned <- paste("data/",baseFile,"_cleaned.txt", sep="")
w2vBin <- paste("data/",baseFile,".bin", sep="")
combinedTexts$text %>% write_lines(w2vInput)

```


## Create a Vector Space Model

The code below is how you actually train your model. There are some parameters you might want to modify, or, if this is your first time training a model, you can also keep the defaults to start. 

You can adjust the number of processors to use on your computer in training the model with the `threads` parameter; this will impact how quickly the model is trained.

The `vectors` parameter allows you to change the dimensionality of your model to include more or fewer dimensions. Higher numbers of dimensions can make your model more precise, but will also increase both training time and the possibility of random errors. A value between 100 and 500 will work for most projects.

The `window` parameter allows you to control the number of words on either side of the target word that the model treats as relevant context; the smaller the window, the closer the context words will be.

The `iter` parameter allows you to control how many times your corpus is read through during model training. If your corpus is on the smaller side, than increasing the number of iterations can improve the reliability of your results.

The `negative_samples` parameter allows you to control the number of negative samples used in training; the model will take less time to train if you use this parameter to modify only a percentage of the weights for the non-context terms in your corpus over each iteration. For smaller datasets, a value between 5 and 20 is recommended; for larger ones, you can use smaller values, between 2 and 5.

For more on these parameters, and other options that you have in training a model, see the [code documentation](https://rdrr.io/github/bmschmidt/wordVectors/man/train_word2vec.html). 


```{r}
THREADS <- 3

# prep_word2vec will prepare your corpus by creating a single text file and cleaning and lowercasing your text with the `tokenizers` package. If you set the value of `bundle_ngrams` to be greater than 1, it will automatically join common bigrams into a single word. 
prep_word2vec(origin=w2vInput, destination=w2vCleaned, lowercase=T, bundle_ngrams=1)

# The code below will train or read-in a model (note that if you want to overwrite an existing model, you will first need to delete the .bin file in your data folder or this code will just read-in the model you already have). See above on how you might modify the parameters before training your model. 
if (!file.exists(w2vBin)) {
  w2vModel <- train_word2vec(
    w2vCleaned,
    output_file=w2vBin,
    vectors=100,
    threads=THREADS,
    window=6, iter=10, negative_samples=15
  )
} else {
  w2vModel <- read.vectors(w2vBin)
}


```


## Visualize 

We can get a glimpse of what the model looks like by plotting it in two dimensions. Keep in mind that the model actually has many more dimensions, so we are, in effect, flattening it. Though the visualization is difficult to read, you should be able to see that similar words—words that are near each other in vector space—tend to clump together. The code below will likely take a minute or two to run, and your results will appear in the "Plots" window to the right (you can hit the "Zoom" button to get a better view).

```{r}
w2vModel %>% plot(perplexity=10)

```


## Clustering

The following script provides a way to cluster words that are near each other in vector space, using the k-means clustering algorithm. Below, we choose 150 `centers`, or 150 points around which to cluster words. Then we select ten random clusters and 15 words from each cluster to view. This code will also take a minute or two to run. You can change the number of centers, number of clusters to view, or the number of words to see—you can also increase the number of iterations (the number of times the algorithm should adjust where the centers are and where terms are positioned in relationship to those centers).

```{r}
centers <- 150
clustering <- kmeans(w2vModel, centers=centers, iter.max=40)

sapply(sample(1:centers, 10), function(n) {
  names(clustering$cluster[clustering$cluster==n][1:15])
})

```


## Closest To

To find the words closest to a particular word in vector space, fill in that term and then run the code below. If you want to see more words, just increase the number. Make sure not to delete the quotation marks, and enter your word in lowercase. 

```{r}
w2vModel %>% closest_to("girl", 30) 

```


## Closest To Two Things

You might also want to see the words closest to a combination of two (or more) words. Notice that this will open a new window with the results because of the `View()` function. If you prefer to see your results in this format, you can paste "%>% View()" at the end of the code above; or, if you prefer to see your results in the console, you can delete "%>% View()" from the code below. Note that the code below also shows just 20 results, instead of 30.

```{r}

w2vModel %>% closest_to(~"girl"+"woman", 20) %>% View()

```


## Closest To The Space Between Two Things

Or, you might want to look at the space between two terms, to see which words are similar to one term but not another. 

```{r}

w2vModel %>% closest_to(~"man"-"woman", 20) %>% View()

```


## Analogies

You can even construct analogies, such as in the example below; these use vector math to subtract the contexts associated with one word from another (such as subtracting "man" from "king") and then add a third term (such as "woman"), which brings you to new vector space where you will find terms associated with the distinction between the first two terms *plus* the contexts of the third term.  Put more simply, this lets you ask questions like "man" is to "king" as "woman" is to *what*? In this example, we might expect to find the term "queen" as a result.

```{r}

w2vModel %>% closest_to(~"king"-"man"+"woman", 20) %>%  View()

```


## Read-in Existing Model Files

If you want to read-in a previously-trained model, you can do so with the code below (just replace "name_of_your_file" with the name of your file, and make sure you don't delete the .bin extension or the quotation marks). As long as you follow all of the instructions above about training your models and setting up the folders for your work in RStudio, all of your trained models will be saved as binary files (with a .bin extension) in your "data" folder. 

If you ever want to over-write a model you've already trained (say, if you realize you need to change something in your input files and then train the model again), make sure to delete that model's .bin file first. 

You can also read-in models trained by others if you save them in your "data" folder and then read them in with the code below. 

After you've restarted RStudio (in addition to checking your working directory and loading your packages), you'll also need to use the code below to read-in your model again. 

```{r}

w2vModel = read.vectors("data/name_of_your_file.bin")

```

## Export Queries
The code below will enable you to export the results from a particular query. To export query results, all you need to do is change the part after "w2vModel %>%" to match the query that you want to export. An example is filled in so that you can see what this looks like; to export the terms closest to a particular query term, you just need to change the text inside of the quotation marks. You can also adjust the number of words in the results set, if you want to see more or fewer. If you'd like to export results from a different query, such as addition or subtraction, just paste over the example query with the one that you want to export. 

The first line of code defines the variable "w2vExport" as whatever query you set. The second line actually exports a csv file (which you can open in any program on your computer that works with tabular data). You can call the file whatever you like by replacing the template text inside of the quotation marks. The csv file will be exported to the "output" folder in your current working directory, and the default is *not* to override any existing files with the same name, so if you do want to override one of those, delete it first. 

```{r}

w2vExport <- w2vModel %>% closest_to("girl", 30) 

#Change "name_of_your_query" to a descriptive name that you want to give to your export file.
write.csv(file="output/name_of_your_query.csv", x=w2vExport)


```

## Export Clusters
You can use a similar method to export your clusters; the code below will first generate a set of clusters and then export a specified (by you) number of terms from those clusters. As above, you can change the number of centers and iterations when you are generating the clusters; you can also change how many sets of clusters and words from each cluster to export. The exporting mechanism is the same as with exporting queries above; you just change the language in the quotation marks to match the name that you want to give your file. 

```{r}

centers <- 150
clustering <- kmeans(w2vModel,centers=centers,iter.max = 40)

#Change "name_of_your_query" to a descriptive name that you want to give to your export file.
w2vExport <-sapply(sample(1:centers,150),function(n) {
  names(clustering$cluster[clustering$cluster==n][1:15])
})

write.csv(file="output/name_of_your_cluster.csv", x=w2vExport)

```
## Evaluate the Model

Below is a very simple test that will calculate the cosine similarities for a small set of word pairs that are likely to be related in many models. You can customize this list for your own corpus by editing the pairs below, or even adding new ones (add as many as you like, but just make sure to follow the same format as in the examples below). This code will produce a "model-test-results.csv" file with cosine similarity scores on these word pairs for every model in your folder (so, if your folder is very crowded, this will take a bit of time to run). The results file will be in the "output" folder of your current working directory, and the default is *not* to override any existing files with the same name, so if you do want to override one of those, delete it first. This is meant to be an example of the kinds of testing that are used in model evaluation, and is not a substitute for more rigorous testing processes. 

There's one additional package you'll need for this test; the line of code below installs it. You don't need to run this code again, after you've installed the package.

```{r}
install.packages("lsa")

```

Now, you can run this test by hitting `command-return` or `control-return` to run one line a time, or just hit the green button in the top right of the code block below. 

```{r}
# This first line just loads that additional package; if you've already loaded it in a particular session, there's no need to run it again.
library(lsa)

files_list  = list.files(pattern="*.bin$", recursive=TRUE)

rownames <- c()

data_frame <- data.frame()
data = list(c("away", "off"),
            c("before", "after"),
            c("cause", "effects"),
            c("children", "parents"),
            c("come", "go"),
            c("day", "night"),
            c("first", "second"),
            c("good", "bad"),
            c("last", "first"),
            c("kind", "sort"),
            c("leave", "quit"),
            c("life", "death"),
            c("girl", "boy"),
            c("little", "small"))

data_list = list()

for(fn in files_list) {
  
  wwp_model = read.vectors(fn)
  sims <- c()
  for(pairs in data)
  {
    vector1 <- c()
    for(x in wwp_model[[pairs[1]]]) {
      vector1 <- c(vector1, x)
    }
    
    vector2 <- c()
    for(x in wwp_model[[pairs[2]]]) {
      vector2 <- c(vector2, x)
    }
    
    sims <- c(sims, cosine(vector1, vector2))
    f_name <- strsplit(fn, "/")[[1]][[2]]
    data_list[[f_name]] <- sims
  }
  
}

for(pairs in data) {
  rownames <- c(rownames, paste(pairs[1], pairs[2], sep="-"))
}

results <- structure(data_list,
                     class     = "data.frame",
                     row.names = rownames
)

#If you want to give your results document a more specific name; you can edit "model-test-results" below. 
write.csv(file="output/model-test-results.csv", x=results)

```


## Credits and Thanks

This tutorial uses the `wordVectors` package developed by Ben Schmidt and Jian Li, itself based on the original `word2vec` code developed by Mikolov et al. The walkthrough was also informed by workshop materials authored by Schmidt, as well as by an exercise created by Thanasis Kinias and Ryan Cordell for the "Humanities Data Analysis" course, and a later version used in Elizabeth Maddock Dillon and Sarah Connell's "Literature and Digital Diversity" class, both at Northeastern University.

This version of the walkthrough was developed as part of the Word Vectors for the Thoughtful Humanist series at Northeastern. Word Vectors for the Thoughtful Humanist has been made possible in part by a major grant from the National Endowment for the Humanities: Exploring the human endeavor. Any views, findings, conclusions, or recommendations expressed in this project, do not necessarily represent those of the National Endowment for the Humanities.


